import streamlit as st
import google.generativeai as genai
import json, re, os, io, base64, tempfile
from google.cloud import vision
from PIL import Image
import pdfplumber

# ===========================
# 1) Ù…ÙØ§ØªÙŠØ­ ÙˆØ±Ø¨Ø· Ø§Ù„Ø®Ø¯Ù…Ø§Øª
# ===========================
GEMINI_KEY = st.secrets.get("GEMINI_API_KEY", None)
VISION_KEY_B64 = st.secrets.get("GOOGLE_VISION_KEY_B64", None)

if GEMINI_KEY:
    genai.configure(api_key=GEMINI_KEY)

if VISION_KEY_B64:
    key_json = base64.b64decode(VISION_KEY_B64).decode("utf-8")
    with tempfile.NamedTemporaryFile(delete=False, suffix=".json") as tmp:
        tmp.write(key_json.encode("utf-8"))
        os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = tmp.name


# ===========================
# 2) OCR Ø´Ø§Ù…Ù„ (ØµÙˆØ± + PDF)
# ===========================
def _vision_client():
    return vision.ImageAnnotatorClient()

def _ocr_image_bytes(client: vision.ImageAnnotatorClient, img_bytes: bytes) -> str:
    image = vision.Image(content=img_bytes)
    resp = client.document_text_detection(image=image)
    if resp.error.message:
        raise RuntimeError(resp.error.message)
    if resp.full_text_annotation and resp.full_text_annotation.text:
        return resp.full_text_annotation.text
    if resp.text_annotations:
        return resp.text_annotations[0].description
    return ""

def extract_text_any(uploaded_file, dpi: int = 200) -> str:
    """
    ÙŠØ¯Ø¹Ù… PDF + ØµÙˆØ± (PNG/JPG). Ù„Ù„Ù€ PDF Ù†ÙØ­ÙˆÙ‘Ù„ ÙƒÙ„ ØµÙØ­Ø© Ø¥Ù„Ù‰ ØµÙˆØ±Ø© Ø«Ù… Ù†Ø·Ø¨Ù‘Ù‚ OCR.
    """
    name = (uploaded_file.name or "").lower()
    uploaded_file.seek(0)
    data = uploaded_file.read()

    client = _vision_client()

    if name.endswith(".pdf"):
        pages_text = []
        with pdfplumber.open(io.BytesIO(data)) as pdf:
            for page in pdf.pages:
                pil = page.to_image(resolution=dpi).original.convert("RGB")
                buf = io.BytesIO()
                pil.save(buf, format="PNG")
                pages_text.append(_ocr_image_bytes(client, buf.getvalue()))
        return ("\n\n--- ØµÙØ­Ø© Ø¬Ø¯ÙŠØ¯Ø© ---\n\n".join(t.strip() for t in pages_text)).strip()
    else:
        pil = Image.open(io.BytesIO(data)).convert("RGB")
        buf = io.BytesIO()
        pil.save(buf, format="PNG")
        return _ocr_image_bytes(client, buf.getvalue())


# ===========================
# 3) Prompt Ø§Ù„ÙˆØ³ÙˆÙ… (Ù…ÙˆØ§Ø¯ ÙÙ‚Ø·)
# ===========================
AGREEMENT_PROMPT_TEMPLATE = r"""
Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ **Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù…ÙˆØ§Ø¯ ÙÙ‚Ø·** Ù…Ù† Ù†Øµ Ø§ØªÙØ§Ù‚ÙŠØ© Ø£Ùˆ Ø¹Ø±Ø¶.
Ø£Ø¹Ø¯ Ø§Ù„Ø±Ø¯ **Ø¨Ø§Ù„Ø¶Ø¨Ø·** Ø¶Ù…Ù† Ø§Ù„ÙˆØ³Ù…ÙŠÙ† Ø§Ù„ØªØ§Ù„ÙŠÙŠÙ†ØŒ ÙˆÙ„Ø§ ØªØ¶Ù Ø£ÙŠ Ù†Øµ Ø®Ø§Ø±Ø¬Ù‡Ù…Ø§:

<<<ITEMS_JSON_ARRAY>>>
[
  {{
    "Ø§Ø³Ù…_Ø§Ù„Ù…Ø§Ø¯Ø©": "Ù…Ø«Ø§Ù„",
    "Ø³Ø¹Ø±_Ø§Ù„Ø´Ø±Ø§Ø¡_Ù‚Ø¨Ù„_Ø§Ù„Ø¶Ø±ÙŠØ¨Ø©": 0.0,
    "Ø³Ø¹Ø±_Ø§Ù„Ø´Ø±Ø§Ø¡_Ù…Ø¹_Ø§Ù„Ø¶Ø±ÙŠØ¨Ø©": 0.0,
    "Ø§Ù„ÙƒÙ…ÙŠØ©_Ø§Ù„Ù…Ø´ØªØ±Ø§Ø©_Ø¨Ø§Ù„Ø­Ø¨Ø©": 0,
    "Ø§Ù„Ù‚ÙŠÙ…Ø©_Ø§Ù„Ù…Ø´ØªØ±Ø§Ø©_Ø¨Ø§Ù„Ø¯ÙŠÙ†Ø§Ø±": 0.0,
    "Ù†Ø³Ø¨Ø©_Ø¶Ø±ÙŠØ¨Ø©_Ø§Ù„Ù…Ø¨ÙŠØ¹Ø§Øª": 0.0
  }}
]
<<<END_ITEMS_JSON_ARRAY>>>

ØªØ¹Ù„ÙŠÙ…Ø§Øª Ù…Ù‡Ù…Ø©:
- Ø£Ø¹ÙØ¯ Ù…ØµÙÙˆÙØ© JSON ØµØ­ÙŠØ­Ø© ÙÙ‚Ø· Ø¯Ø§Ø®Ù„ <<<ITEMS_JSON_ARRAY>>>â€¦<<<END_ITEMS_JSON_ARRAY>>>.
- "Ø§Ø³Ù…_Ø§Ù„Ù…Ø§Ø¯Ø©" Ù†Øµ Ø¥Ø¬Ø¨Ø§Ø±ÙŠ.
- Ø¨Ø§Ù‚ÙŠ Ø§Ù„Ø­Ù‚ÙˆÙ„ Ù‚ÙŠÙ… Ø±Ù‚Ù…ÙŠØ© (Ø¹Ø´Ø±ÙŠØ©) Ø¨Ø§Ù„Ø¯ÙŠÙ†Ø§Ø± Ø¨Ø¹Ø¯ Ø¯Ù…Ø¬ Ø§Ù„Ø¯ÙŠÙ†Ø§Ø± + Ø§Ù„ÙÙ„Ø³/1000 Ø¥Ù† Ø¸Ù‡Ø±Øª Ù…Ù†ÙØµÙ„Ø©.
- "Ø§Ù„ÙƒÙ…ÙŠØ©_Ø§Ù„Ù…Ø´ØªØ±Ø§Ø©_Ø¨Ø§Ù„Ø­Ø¨Ø©" Ø±Ù‚Ù… ØµØ­ÙŠØ­.
- "Ù†Ø³Ø¨Ø©_Ø¶Ø±ÙŠØ¨Ø©_Ø§Ù„Ù…Ø¨ÙŠØ¹Ø§Øª" ÙƒÙ‚ÙŠÙ…Ø© Ø¹Ø´Ø±ÙŠØ© (Ù…Ø«Ù„Ø§Ù‹ 0.16 ÙˆÙ„ÙŠØ³ 16%).
- Ù„Ø§ ØªØ¹Ù„ÙŠÙ‚Ø§ØªØŒ Ù„Ø§ Ø£Ø³Ø·Ø± Ø´Ø±Ø­ØŒ Ù„Ø§ ÙÙˆØ§ØµÙ„ Ø²Ø§Ø¦Ø¯Ø© Ù‚Ø¨Ù„ ] Ø£Ùˆ }}.
- Ø¥Ù† Ù„Ù… ØªÙˆØ¬Ø¯ Ù…ÙˆØ§Ø¯ØŒ Ø£Ø¹Ø¯ [].

Ø§Ù„Ù†Øµ:
----------------
{doc_text}
"""


# ===========================
# 4) ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙˆØ³ÙˆÙ… + ØªÙ†Ø¸ÙŠÙ JSON Ø§Ù„Ù…ÙˆØ§Ø¯
# ===========================
def parse_tagged_response(raw: str) -> dict:
    import json, re
    raw = re.sub(r"[\u200E\u200F\u202A-\u202E\u2066-\u2069\uFEFF\u200B\u200C\u200D]", "", raw or "").strip()

    m = re.search(r"<<<ITEMS_JSON_ARRAY>>>(.*?)<<<END_ITEMS_JSON_ARRAY>>>", raw, flags=re.S)
    items_json = (m.group(1).strip() if m else "")

    items = []
    if items_json:
        # Ø¥Ø²Ø§Ù„Ø© code fences Ø¥Ù† ÙˆÙØ¬Ø¯Øª
        items_json = re.sub(r"^```(?:json)?\s*|\s*```$", "", items_json, flags=re.I | re.M).strip()
        # ØªØ·Ø¨ÙŠØ¹ Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„Ø§Ù‚ØªØ¨Ø§Ø³ ÙˆØ§Ù„ÙÙˆØ§ØµÙ„ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
        items_json = (items_json
                      .replace("â€œ", '"').replace("â€", '"').replace("â€™", "'").replace("â€˜", "'")
                      .replace("ØŒ", ",").replace("Ù«", "."))
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ÙÙˆØ§ØµÙ„ Ø§Ù„Ø²Ø§Ø¦Ø¯Ø© Ù‚Ø¨Ù„ Ø§Ù„Ø£Ù‚ÙˆØ§Ø³
        items_json = re.sub(r",\s*([}\]])", r"\1", items_json)
        # Ø§Ù‚ØªØ¨Ø§Ø³ Ø§Ù„Ù…ÙØ§ØªÙŠØ­ ØºÙŠØ± Ø§Ù„Ù…Ù‚ØªØ¨Ø³Ø©
        items_json = re.sub(r'([{,]\s*)([A-Za-z0-9_Ø¡-ÙŠ]+)\s*:', r'\1"\2":', items_json)
        # Ø§Ø³Ù…_Ø§Ù„Ù…Ø§Ø¯Ø© ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ù†ØµÙ‹Ø§
        items_json = re.sub(r'("Ø§Ø³Ù…_Ø§Ù„Ù…Ø§Ø¯Ø©"\s*:\s*)(-?\d+(?:\.\d+)?)', r'\1"\2"', items_json)

        try:
            parsed = json.loads(items_json)
        except Exception:
            try:
                parsed = json.loads(re.sub(r"\s+\n\s+", "\n", items_json))
            except Exception:
                parsed = []

        if isinstance(parsed, dict):
            items = [parsed]
        elif isinstance(parsed, list):
            items = [x for x in parsed if isinstance(x, dict)]
        else:
            items = []
    return {"Ø§Ù„Ù…ÙˆØ§Ø¯": items}


# ===========================
# 4.1) Ø£Ø¯ÙˆØ§Øª Ø§Ù„ØªØ¬Ø²Ø¦Ø© ÙˆØ§Ù„Ø¯Ù…Ø¬ + Ø§Ù„ÙØ§Ù„Ù’Ø¨Ø§ÙƒØ³
# ===========================
def chunk_text(text: str, max_chars: int = 10000) -> list:
    text = text or ""
    if len(text) <= max_chars:
        return [text]
    chunks, start = [], 0
    while start < len(text):
        end = min(len(text), start + max_chars)
        nl = text.rfind("\n", start, end)
        if nl == -1 or nl <= start + int(max_chars * 0.5):
            nl = end
        chunk = text[start:nl].strip()
        if chunk:
            chunks.append(chunk)
        start = nl
    return chunks

def merge_items_only(parts: list) -> dict:
    merged = {"Ø§Ù„Ù…ÙˆØ§Ø¯": []}
    for p in parts or []:
        its = p.get("Ø§Ù„Ù…ÙˆØ§Ø¯") or []
        merged["Ø§Ù„Ù…ÙˆØ§Ø¯"].extend([x for x in its if isinstance(x, dict)])
    return merged

def _sanitize_model_name(name: str) -> str:
    return (name or "").replace("models/", "").strip()

def _available_fallbacks(selected: str) -> list:
    """
    Ù…Ù‚ØªØµØ± Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø© ÙÙŠ Ø­Ø³Ø§Ø¨Ùƒ (2.5/2.0).
    Ø¹Ø¯Ù‘Ù„ Ø§Ù„ØªØ±ØªÙŠØ¨ Ø¥Ø°Ø§ ØªØ­Ø¨ Ø§Ù„Ø¯Ù‚Ø© (pro) Ø£Ùˆ Ø§Ù„Ø³Ø±Ø¹Ø© (flash/lite).
    """
    wanted = []
    sel = _sanitize_model_name(selected)
    def add(m):
        m = _sanitize_model_name(m)
        if m and m not in wanted:
            wanted.append(m)

    add(sel)
    add("gemini-2.5-pro")
    add("gemini-2.5-flash")
    add("gemini-2.5-flash-lite")
    add("gemini-2.0-flash")
    add("gemini-2.0-flash-lite")
    add("gemini-2.0-flash-exp")
    return wanted


# ===========================
# 4.2) ØªÙ†Ø³ÙŠÙ‚ Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ù…ÙˆØ§Ø¯ (ØªÙ†Ø¸ÙŠÙ Ø£Ø±Ù‚Ø§Ù… + Ø£Ø¹Ù…Ø¯Ø© Ù…Ø´ØªÙ‚Ø© + Ø¹Ø±Ø¶)
# ===========================
def _arabic_digits_to_western(s: str) -> str:
    if not isinstance(s, str):
        s = str(s)
    trans = str.maketrans("Ù Ù¡Ù¢Ù£Ù¤Ù¥Ù¦Ù§Ù¨Ù©", "0123456789")
    return s.translate(trans)

def _to_float(val):
    if val is None:
        return None
    if isinstance(val, (int, float)):
        return float(val)
    s = str(val).strip()
    if not s:
        return None
    s = _arabic_digits_to_western(s)
    s = s.replace("ØŒ", ",").replace("Ù«", ".")
    s = re.sub(r"(Ø¯ÙŠÙ†Ø§Ø±|JD|Ø¯\.|ÙÙ„Ø³|Ø¶Ø±ÙŠØ¨Ø©|%)", "", s, flags=re.I).strip()
    s = s.replace(",", "").replace(" ", "")
    m = re.search(r"-?\d+(?:\.\d+)?", s)
    if not m:
        return None
    try:
        return float(m.group(0))
    except Exception:
        return None

def normalize_items_table(items: list):
    import pandas as pd
    cols = [
        "Ø§Ø³Ù…_Ø§Ù„Ù…Ø§Ø¯Ø©",
        "Ø³Ø¹Ø±_Ø§Ù„Ø´Ø±Ø§Ø¡_Ù‚Ø¨Ù„_Ø§Ù„Ø¶Ø±ÙŠØ¨Ø©",
        "Ø³Ø¹Ø±_Ø§Ù„Ø´Ø±Ø§Ø¡_Ù…Ø¹_Ø§Ù„Ø¶Ø±ÙŠØ¨Ø©",
        "Ø§Ù„ÙƒÙ…ÙŠØ©_Ø§Ù„Ù…Ø´ØªØ±Ø§Ø©_Ø¨Ø§Ù„Ø­Ø¨Ø©",
        "Ø§Ù„Ù‚ÙŠÙ…Ø©_Ø§Ù„Ù…Ø´ØªØ±Ø§Ø©_Ø¨Ø§Ù„Ø¯ÙŠÙ†Ø§Ø±",
        "Ù†Ø³Ø¨Ø©_Ø¶Ø±ÙŠØ¨Ø©_Ø§Ù„Ù…Ø¨ÙŠØ¹Ø§Øª",
    ]
    norm_rows = []
    for it in (items or []):
        row = {k: it.get(k) for k in cols}
        norm_rows.append(row)
    df = pd.DataFrame(norm_rows, columns=cols)

    for c in ["Ø³Ø¹Ø±_Ø§Ù„Ø´Ø±Ø§Ø¡_Ù‚Ø¨Ù„_Ø§Ù„Ø¶Ø±ÙŠØ¨Ø©", "Ø³Ø¹Ø±_Ø§Ù„Ø´Ø±Ø§Ø¡_Ù…Ø¹_Ø§Ù„Ø¶Ø±ÙŠØ¨Ø©", "Ø§Ù„ÙƒÙ…ÙŠØ©_Ø§Ù„Ù…Ø´ØªØ±Ø§Ø©_Ø¨Ø§Ù„Ø­Ø¨Ø©",
              "Ø§Ù„Ù‚ÙŠÙ…Ø©_Ø§Ù„Ù…Ø´ØªØ±Ø§Ø©_Ø¨Ø§Ù„Ø¯ÙŠÙ†Ø§Ø±", "Ù†Ø³Ø¨Ø©_Ø¶Ø±ÙŠØ¨Ø©_Ø§Ù„Ù…Ø¨ÙŠØ¹Ø§Øª"]:
        df[c] = df[c].apply(_to_float)

    df["Ù†Ø³Ø¨Ø©_Ø¶Ø±ÙŠØ¨Ø©_Ø§Ù„Ù…Ø¨ÙŠØ¹Ø§Øª"] = df["Ù†Ø³Ø¨Ø©_Ø¶Ø±ÙŠØ¨Ø©_Ø§Ù„Ù…Ø¨ÙŠØ¹Ø§Øª"].apply(
        lambda x: (x/100.0) if (x is not None and 1.0 <= x <= 100.0) else x
    )

    df["Ù‚ÙŠÙ…Ø©_Ù‚Ø¨Ù„_Ø§Ù„Ø¶Ø±ÙŠØ¨Ø©_(Ø­Ø³Ø§Ø¨)"] = (
        (df["Ø³Ø¹Ø±_Ø§Ù„Ø´Ø±Ø§Ø¡_Ù‚Ø¨Ù„_Ø§Ù„Ø¶Ø±ÙŠØ¨Ø©"].fillna(0.0)) * (df["Ø§Ù„ÙƒÙ…ÙŠØ©_Ø§Ù„Ù…Ø´ØªØ±Ø§Ø©_Ø¨Ø§Ù„Ø­Ø¨Ø©"].fillna(0.0))
    )
    df["Ù‚ÙŠÙ…Ø©_Ù…Ø¹_Ø§Ù„Ø¶Ø±ÙŠØ¨Ø©_(Ø­Ø³Ø§Ø¨)"] = (
        (df["Ø³Ø¹Ø±_Ø§Ù„Ø´Ø±Ø§Ø¡_Ù…Ø¹_Ø§Ù„Ø¶Ø±ÙŠØ¨Ø©"].fillna(0.0)) * (df["Ø§Ù„ÙƒÙ…ÙŠØ©_Ø§Ù„Ù…Ø´ØªØ±Ø§Ø©_Ø¨Ø§Ù„Ø­Ø¨Ø©"].fillna(0.0))
    )

    df["Ø§Ù„Ù‚ÙŠÙ…Ø©_Ø§Ù„Ù…Ø´ØªØ±Ø§Ø©_Ø¨Ø§Ù„Ø¯ÙŠÙ†Ø§Ø±"] = df["Ø§Ù„Ù‚ÙŠÙ…Ø©_Ø§Ù„Ù…Ø´ØªØ±Ø§Ø©_Ø¨Ø§Ù„Ø¯ÙŠÙ†Ø§Ø±"].fillna(
        df["Ù‚ÙŠÙ…Ø©_Ù…Ø¹_Ø§Ù„Ø¶Ø±ÙŠØ¨Ø©_(Ø­Ø³Ø§Ø¨)"].where(df["Ù‚ÙŠÙ…Ø©_Ù…Ø¹_Ø§Ù„Ø¶Ø±ÙŠØ¨Ø©_(Ø­Ø³Ø§Ø¨)"] > 0, df["Ù‚ÙŠÙ…Ø©_Ù‚Ø¨Ù„_Ø§Ù„Ø¶Ø±ÙŠØ¨Ø©_(Ø­Ø³Ø§Ø¨)"])
    )

    display_cols = [
        "Ø§Ø³Ù…_Ø§Ù„Ù…Ø§Ø¯Ø©",
        "Ø§Ù„ÙƒÙ…ÙŠØ©_Ø§Ù„Ù…Ø´ØªØ±Ø§Ø©_Ø¨Ø§Ù„Ø­Ø¨Ø©",
        "Ø³Ø¹Ø±_Ø§Ù„Ø´Ø±Ø§Ø¡_Ù‚Ø¨Ù„_Ø§Ù„Ø¶Ø±ÙŠØ¨Ø©",
        "Ø³Ø¹Ø±_Ø§Ù„Ø´Ø±Ø§Ø¡_Ù…Ø¹_Ø§Ù„Ø¶Ø±ÙŠØ¨Ø©",
        "Ù†Ø³Ø¨Ø©_Ø¶Ø±ÙŠØ¨Ø©_Ø§Ù„Ù…Ø¨ÙŠØ¹Ø§Øª",
        "Ù‚ÙŠÙ…Ø©_Ù‚Ø¨Ù„_Ø§Ù„Ø¶Ø±ÙŠØ¨Ø©_(Ø­Ø³Ø§Ø¨)",
        "Ù‚ÙŠÙ…Ø©_Ù…Ø¹_Ø§Ù„Ø¶Ø±ÙŠØ¨Ø©_(Ø­Ø³Ø§Ø¨)",
        "Ø§Ù„Ù‚ÙŠÙ…Ø©_Ø§Ù„Ù…Ø´ØªØ±Ø§Ø©_Ø¨Ø§Ù„Ø¯ÙŠÙ†Ø§Ø±",
    ]
    totals = {
        "Ø¥Ø¬Ù…Ø§Ù„ÙŠ_Ø§Ù„ÙƒÙ…ÙŠØ©": float(df["Ø§Ù„ÙƒÙ…ÙŠØ©_Ø§Ù„Ù…Ø´ØªØ±Ø§Ø©_Ø¨Ø§Ù„Ø­Ø¨Ø©"].fillna(0).sum()),
        "Ø¥Ø¬Ù…Ø§Ù„ÙŠ_Ù‚ÙŠÙ…Ø©_Ù‚Ø¨Ù„_Ø§Ù„Ø¶Ø±ÙŠØ¨Ø©_(Ø­Ø³Ø§Ø¨)": float(df["Ù‚ÙŠÙ…Ø©_Ù‚Ø¨Ù„_Ø§Ù„Ø¶Ø±ÙŠØ¨Ø©_(Ø­Ø³Ø§Ø¨)"].fillna(0).sum()),
        "Ø¥Ø¬Ù…Ø§Ù„ÙŠ_Ù‚ÙŠÙ…Ø©_Ù…Ø¹_Ø§Ù„Ø¶Ø±ÙŠØ¨Ø©_(Ø­Ø³Ø§Ø¨)": float(df["Ù‚ÙŠÙ…Ø©_Ù…Ø¹_Ø§Ù„Ø¶Ø±ÙŠØ¨Ø©_(Ø­Ø³Ø§Ø¨)"].fillna(0).sum()),
        "Ø¥Ø¬Ù…Ø§Ù„ÙŠ_Ø§Ù„Ù‚ÙŠÙ…Ø©_Ø§Ù„Ù…Ø´ØªØ±Ø§Ø©_Ø¨Ø§Ù„Ø¯ÙŠÙ†Ø§Ø±": float(df["Ø§Ù„Ù‚ÙŠÙ…Ø©_Ø§Ù„Ù…Ø´ØªØ±Ø§Ø©_Ø¨Ø§Ù„Ø¯ÙŠÙ†Ø§Ø±"].fillna(0).sum()),
    }
    return df[display_cols], totals

def render_items_table(items: list, title: str = "ğŸ“¦ Ø§Ù„Ù…ÙˆØ§Ø¯ Ø¶Ù…Ù† Ø§Ù„Ø§ØªÙØ§Ù‚ÙŠØ©"):
    import pandas as pd
    st.markdown('<div class="card">', unsafe_allow_html=True)
    st.markdown(f'<div class="section-title">{title}</div>', unsafe_allow_html=True)

    df, totals = normalize_items_table(items)

    if df.empty:
        st.info("Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…ÙˆØ§Ø¯ Ù…Ø³ØªØ®Ø±Ø¬Ø©.")
        st.markdown('</div>', unsafe_allow_html=True)
        return

    st.dataframe(df, use_container_width=True, height=420)

    c1, c2, c3, c4 = st.columns(4)
    c1.metric("Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ÙƒÙ…ÙŠØ©", f"{totals['Ø¥Ø¬Ù…Ø§Ù„ÙŠ_Ø§Ù„ÙƒÙ…ÙŠØ©']:.0f}")
    c2.metric("Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ù‚Ø¨Ù„ Ø§Ù„Ø¶Ø±ÙŠØ¨Ø©", f"{totals['Ø¥Ø¬Ù…Ø§Ù„ÙŠ_Ù‚ÙŠÙ…Ø©_Ù‚Ø¨Ù„_Ø§Ù„Ø¶Ø±ÙŠØ¨Ø©_(Ø­Ø³Ø§Ø¨)']:.3f}")
    c3.metric("Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ù…Ø¹ Ø§Ù„Ø¶Ø±ÙŠØ¨Ø©", f"{totals['Ø¥Ø¬Ù…Ø§Ù„ÙŠ_Ù‚ÙŠÙ…Ø©_Ù…Ø¹_Ø§Ù„Ø¶Ø±ÙŠØ¨Ø©_(Ø­Ø³Ø§Ø¨)']:.3f}")
    c4.metric("Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù‚ÙŠÙ…Ø© Ø§Ù„Ù…Ø¯Ø®Ù„Ø©", f"{totals['Ø¥Ø¬Ù…Ø§Ù„ÙŠ_Ø§Ù„Ù‚ÙŠÙ…Ø©_Ø§Ù„Ù…Ø´ØªØ±Ø§Ø©_Ø¨Ø§Ù„Ø¯ÙŠÙ†Ø§Ø±']:.3f}")

    csv_bytes = df.to_csv(index=False).encode("utf-8")
    st.download_button("â¬‡ï¸ ØªÙ†Ø²ÙŠÙ„ CSV", data=csv_bytes, file_name="items.csv", mime="text/csv")

    try:
        import io
        buf = io.BytesIO()
        with pd.ExcelWriter(buf, engine="xlsxwriter") as writer:
            df.to_excel(writer, index=False, sheet_name="items")
        st.download_button("â¬‡ï¸ ØªÙ†Ø²ÙŠÙ„ Excel", data=buf.getvalue(),
                           file_name="items.xlsx",
                           mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")
    except Exception:
        pass

    st.markdown('</div>', unsafe_allow_html=True)


# ===========================
# 5) ØªØ­Ù„ÙŠÙ„ Ø¨Ø§Ù„Ù€ Gemini (Ù…Ø­Ø§ÙˆÙ„Ø© ÙƒØ§Ù…Ù„Ø© Ø«Ù… ØªØ¬Ø²Ø¦Ø©)
# ===========================
def analyze_agreement_with_gemini(text: str, selected_model: str, debug: bool = False) -> dict:
    """
    Ù†Ø­Ø§ÙˆÙ„ ØªØ­Ù„ÙŠÙ„ ÙƒØ§Ù…Ù„ Ø§Ù„Ù†Øµ. Ø¥Ø°Ø§ ÙØ´Ù„ Ø£Ùˆ Ø¬Ø§Ø¡ Ø±Ø¯ Ù…Ø­Ø¬ÙˆØ¨ØŒ Ù†Ø¬Ø²Ù‘Ø¦ Ø§Ù„Ù†Øµ ÙˆÙ†Ø­Ø§ÙˆÙ„ Ø«Ù… Ù†Ø¯Ù…Ø¬ "Ø§Ù„Ù…ÙˆØ§Ø¯" ÙÙ‚Ø·.
    """
    prompt_full = AGREEMENT_PROMPT_TEMPLATE.format(doc_text=text)

    def run_once(model_name: str, prompt: str) -> str:
        model_name = _sanitize_model_name(model_name)
        model = genai.GenerativeModel(model_name=model_name)
        resp = model.generate_content(
            prompt,
            generation_config={"temperature": 0.15, "max_output_tokens": 8192},
        )
        # Ù„Ø§ ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ resp.textØ› Ø§Ø³ØªØ®Ø±Ø¬ Ù…Ù† candidates/parts ÙˆØªØ¬Ø§ÙˆØ² Ø§Ù„Ø±Ø¯ÙˆØ¯ Ø§Ù„Ù…Ø­Ø¬ÙˆØ¨Ø© (finish_reason=2)
        texts = []
        for cand in getattr(resp, "candidates", []) or []:
            fr = getattr(cand, "finish_reason", None)
            if fr is not None and int(fr) == 2:
                continue
            content = getattr(cand, "content", None)
            if content and getattr(content, "parts", None):
                for p in content.parts:
                    t = getattr(p, "text", None)
                    if t:
                        texts.append(t)
        return "\n".join(texts).strip()

    # 1) Ù…Ø­Ø§ÙˆÙ„Ø© ÙƒØ§Ù…Ù„Ø© Ù…Ø¹ fallback Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©
    for m in _available_fallbacks(selected_model):
        try:
            raw = run_once(m, prompt_full)
            if debug:
                st.caption(f"ğŸ“„ Raw (full) from {m}:")
                st.code((raw or "")[:1200] + ("..." if raw and len(raw) > 1200 else ""))
            parsed = parse_tagged_response(raw)
            # Ù†Ø¬Ø§Ø­ Ù„Ùˆ Ø¹Ù†Ø¯Ù†Ø§ Ù…ÙˆØ§Ø¯ (Ø­ØªÙ‰ Ù„Ùˆ ÙØ§Ø¶ÙŠØ© Ø¨Ù†Ø¬Ø±Ù‘Ø¨ Ø§Ù„ØªØ¬Ø²Ø¦Ø©)
            if parsed.get("Ø§Ù„Ù…ÙˆØ§Ø¯"):
                return parsed
        except Exception as e:
            if debug:
                st.warning(f"âš ï¸ ÙØ´Ù„ Ù…Ø­Ø§ÙˆÙ„Ø© ÙƒØ§Ù…Ù„Ø© Ø¹Ù„Ù‰ {m}: {type(e).__name__}: {e}")

    # 2) Ø®Ø·Ø© (Ø¨): ØªØ¬Ø²Ø¦Ø© Ø§Ù„Ù†Øµ
    chunks = chunk_text(text, max_chars=10000)
    parts = []
    for idx, ch in enumerate(chunks, 1):
        prompt_chunk = AGREEMENT_PROMPT_TEMPLATE.format(doc_text=ch)
        ok = False
        for m in _available_fallbacks(selected_model):
            try:
                raw = run_once(m, prompt_chunk)
                if debug:
                    st.caption(f"ğŸ“„ Raw (chunk {idx}/{len(chunks)}) from {m}:")
                    st.code((raw or "")[:800] + ("..." if raw and len(raw) > 800 else ""))
                parsed = parse_tagged_response(raw)
                parts.append(parsed)
                ok = True
                break
            except Exception as e:
                if debug:
                    st.warning(f"âš ï¸ ÙØ´Ù„ chunk {idx} Ø¹Ù„Ù‰ {m}: {type(e).__name__}: {e}")
                continue
        if not ok and debug:
            st.error(f"âŒ Ù„Ù… Ù†Ù†Ø¬Ø­ ÙÙŠ chunk {idx}")

    if not parts:
        raise RuntimeError("ÙØ´Ù„ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø¹Ø¨Ø± Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„Ø§Øª (ÙƒØ§Ù…Ù„ + Ù…Ø¬Ø²Ø£).")

    return merge_items_only(parts)


# ===========================
# 6) ÙˆØ§Ø¬Ù‡Ø© Streamlit
# ===========================
st.set_page_config(page_title="ØªØ­Ù„ÙŠÙ„ Ø§ØªÙØ§Ù‚ÙŠØ§Øª Ø§Ù„Ù…Ø¤Ø³Ø³Ø© Ø§Ù„Ø§Ø³ØªÙ‡Ù„Ø§ÙƒÙŠØ© Ø§Ù„Ø¹Ø³ÙƒØ±ÙŠØ©", layout="wide")

# Ù„Ù…Ø³Ø© CSS Ø¨Ø³ÙŠØ·Ø©
st.markdown("""
<style>
.section-title{font-weight:700;font-size:1.1rem;margin:8px 0 12px}
.card{background:#ffffff;border:1px solid #eee;border-radius:12px;padding:12px;margin-bottom:14px}
</style>
""", unsafe_allow_html=True)

st.title("ğŸ“‘ Ù†Ø¸Ø§Ù… ØªØ­Ù„ÙŠÙ„ Ø§ØªÙØ§Ù‚ÙŠØ§Øª Ø§Ù„Ù…Ø¤Ø³Ø³Ø© Ø§Ù„Ø§Ø³ØªÙ‡Ù„Ø§ÙƒÙŠØ© Ø§Ù„Ø¹Ø³ÙƒØ±ÙŠØ©")
st.markdown("Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… **Google Vision OCR + Gemini AI** â€” Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ÙˆØ§Ø¯ ÙÙ‚Ø·")

uploaded = st.file_uploader("ğŸ“¤ Ø§Ø±ÙØ¹ ØµÙˆØ±Ø© Ø£Ùˆ Ù…Ù„Ù PDF", type=["png", "jpg", "jpeg", "pdf"])

if uploaded and st.button("ğŸ“„ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ"):
    try:
        with st.spinner("Ø¬Ø§Ø±Ù Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ Ø¹Ø¨Ø± Google Vision..."):
            text = extract_text_any(uploaded)
        st.session_state["ocr_text"] = text
        st.success("âœ… ØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ!")
    except Exception as e:
        st.error(f"âŒ ÙØ´Ù„ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ: {e}")

# Ø¹Ø±Ø¶ Ø§Ù„Ù†Øµ
st.text_area("ğŸ“ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬:", st.session_state.get("ocr_text", ""), height=300)

# Ø¥Ø¹Ø¯Ø§Ø¯ Gemini
if GEMINI_KEY:
    st.success("âœ… Ù…ÙØªØ§Ø­ Gemini ØµØ§Ù„Ø­.")
    try:
        # ÙŠÙ…ÙƒÙ†Ùƒ ØªØ¬Ø§Ù‡Ù„ list_models ÙˆØ§Ù„Ø§ÙƒØªÙØ§Ø¡ Ø¨Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø«Ø§Ø¨ØªØ© Ø¨Ø§Ù„Ø£Ø³ÙÙ„
        _ = genai.list_models()
        models = [
            "gemini-2.5-pro",
            "gemini-2.5-flash",
            "gemini-2.5-flash-lite",
            "gemini-2.0-flash",
            "gemini-2.0-flash-lite",
            "gemini-2.0-flash-exp",
        ]
    except Exception:
        models = [
            "gemini-2.5-pro",
            "gemini-2.5-flash",
            "gemini-2.5-flash-lite",
            "gemini-2.0-flash",
            "gemini-2.0-flash-lite",
            "gemini-2.0-flash-exp",
        ]
    selected_model = st.selectbox("Ø§Ø®ØªØ± Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„:", models, index=0)
    selected_model = _sanitize_model_name(selected_model)
else:
    st.error("âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…ÙØªØ§Ø­ Gemini")
    selected_model = None

debug = st.toggle("ğŸ§  Ø¥Ø¸Ù‡Ø§Ø± Ù…Ø®Ø±Ø¬Ø§Øª Ø§Ù„ØªØ´Ø®ÙŠØµ (Raw)")

if "ocr_text" in st.session_state and selected_model and st.button("ØªØ­Ù„ÙŠÙ„ (Ù…ÙˆØ§Ø¯ ÙÙ‚Ø·)"):
    try:
        result = analyze_agreement_with_gemini(st.session_state["ocr_text"], selected_model, debug)
        st.success("âœ… ØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ÙˆØ§Ø¯")
        # Ø¹Ø±Ø¶ Ø§Ù„Ù…ÙˆØ§Ø¯ ÙÙ‚Ø·
        render_items_table(result.get("Ø§Ù„Ù…ÙˆØ§Ø¯", []) or [], title="ğŸ“¦ Ø§Ù„Ù…ÙˆØ§Ø¯ Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø©")
        # Ø®ÙŠØ§Ø±: Ø¥Ø¸Ù‡Ø§Ø± JSON Ù„Ù„Ù…ÙˆØ§Ø¯ ÙÙ‚Ø·
        with st.expander("ğŸ” JSON Ø§Ù„Ù…ÙˆØ§Ø¯"):
            st.json(result.get("Ø§Ù„Ù…ÙˆØ§Ø¯", []))
    except Exception as e:
        st.error(f"âŒ ÙØ´Ù„ Ø§Ù„ØªØ­Ù„ÙŠÙ„: {e}")
